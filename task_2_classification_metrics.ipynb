{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimenting classification metrics\n",
    "    The metrics that we apply to evaluate our model are very important.Choice of metrics defines how  the performance of one model is compared to that of another. They influence how we weight the statistical importance of one feature as compared to other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this task we will be implimenting following metrics:\n",
    "    - Classification Accuracy.\n",
    "    - Log Loss.\n",
    "    - Area Under ROC Curve.\n",
    "    - Confusion Matrix.\n",
    "    - F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy \n",
    "- is the number of correct predictions made as a ratio of all predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = \n",
    "y_pred_tree = \n",
    "y_true_logistic = \n",
    "y_true_tree ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logistic_classification_acc = ()*100\n",
    "y_tree_classification_acc = ()*100\n",
    "print(\"classificaton accuracy logistic \",y_logistic_classification_acc)\n",
    "print(\"classificaton accuracy tree \",y_tree_classification_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss\n",
    "- Logistic loss is a performance metric for evaluating the predictions of probabilities which states how how much confidence a class can be predicted.\n",
    "- H(P) = â€“ sum x on X p(x) * log(p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logistic_classificaion_log_loss = \n",
    "y_tree_classificaion_log_loss = \n",
    "print(\"classificaton accuracy logistic \",y_logistic_classificaion_log_loss)\n",
    "print(\"classificaton accuracy tree \",y_tree_classificaion_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve\n",
    "- A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The area under the curve is then the approximate integral under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate\n",
    "# false positive\n",
    "fp_tree = \n",
    "# true negative\n",
    "tn_tree = \n",
    "fpr_tree = \n",
    "\n",
    "# True positive rate\n",
    "# false positive\n",
    "tp_tree = \n",
    "# true negative\n",
    "fn_tree = \n",
    "tpr_tree = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate\n",
    "# false positive\n",
    "fp_logistic = \n",
    "# true negative\n",
    "tn_logistic = \n",
    "fpr_logistic = \n",
    "\n",
    "# True positive rate\n",
    "# false positive\n",
    "tp_logistic = \n",
    "# true negative\n",
    "fn_logistic = \n",
    "tpr_logistic = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot a graph between tpr_tree and fpr_tree and compare it with tpr_logistic and fpr_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "- A confusion matrix is a technique for summarizing the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "fp_tree = \n",
    "# true negative\n",
    "tn_tree = \n",
    "# false positive\n",
    "tp_tree = \n",
    "# true negative\n",
    "fn_tree = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "fp_logistic = \n",
    "# true negative\n",
    "tn_logistic = \n",
    "# false positive\n",
    "tp_logistic = \n",
    "# true negative\n",
    "fn_logistic = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "- To find the best balance between precision and recall. F1 score is a harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_tree = \n",
    "f1_score_logistic = \n",
    "print(\"F1 score tree \",f1_score_tree)\n",
    "print(\"F1 score logistic \",f1_score_logistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
