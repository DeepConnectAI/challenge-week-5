{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing classification metrics\n",
    "The metrics that we apply to evaluate our model are very important. The choice of metrics determines the difference of performance between various models one works with. They influence how we weigh the statistical importance of one or more features as compared to other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this task we will be implimenting following metrics:\n",
    "   - Classification Accuracy.\n",
    "   - Log Loss.\n",
    "   - Area Under ROC Curve.\n",
    "   - Confusion Matrix.\n",
    "   - F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy \n",
    "- is the ratio of correctly predicted values to the actual values of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = \n",
    "y_pred_tree = \n",
    "y_true_logistic = \n",
    "y_true_tree ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logistic_classification_acc = ()*100\n",
    "y_tree_classification_acc = ()*100\n",
    "print(\"classificaton accuracy logistic \",y_logistic_classification_acc)\n",
    "print(\"classificaton accuracy tree \",y_tree_classification_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss\n",
    "- Logistic loss is a performance metric for evaluating the predictions of probabilities based on with how much confidence a class can be predicted.\n",
    "- H(P) = â€“ sum x on X p(x) * log(p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logistic_classificaion_log_loss = \n",
    "y_tree_classificaion_log_loss = \n",
    "print(\"classificaton accuracy logistic \",y_logistic_classificaion_log_loss)\n",
    "print(\"classificaton accuracy tree \",y_tree_classificaion_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve\n",
    "- A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to classify the predictions as labels. The area under the curve is then the approximate integral under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate\n",
    "# false positive\n",
    "fp_tree = \n",
    "# true negative\n",
    "tn_tree = \n",
    "fpr_tree = \n",
    "\n",
    "# True positive rate\n",
    "# false positive\n",
    "tp_tree = \n",
    "# true negative\n",
    "fn_tree = \n",
    "tpr_tree = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate\n",
    "# false positive\n",
    "fp_logistic = \n",
    "# true negative\n",
    "tn_logistic = \n",
    "fpr_logistic = \n",
    "\n",
    "# True positive rate\n",
    "# false positive\n",
    "tp_logistic = \n",
    "# true negative\n",
    "fn_logistic = \n",
    "tpr_logistic = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a graph between tpr_tree and fpr_tree and compare it with tpr_logistic and fpr_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ba02f2538f6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot()\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "- A confusion matrix is a technique for summarizing the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "fp_tree = \n",
    "# true negative\n",
    "tn_tree = \n",
    "# false positive\n",
    "tp_tree = \n",
    "# true negative\n",
    "fn_tree = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive\n",
    "fp_logistic = \n",
    "# true negative\n",
    "tn_logistic = \n",
    "# false positive\n",
    "tp_logistic = \n",
    "# true negative\n",
    "fn_logistic = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "- To find the best balance between precision and recall. F1 score is a harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_tree = \n",
    "f1_score_logistic = \n",
    "print(\"F1 score tree \",f1_score_tree)\n",
    "print(\"F1 score logistic \",f1_score_logistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
