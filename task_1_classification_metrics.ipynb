{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "\n",
    "Choosing right evaluation metrics for the problem is one of the most important aspect of machine learning. Choice of metrics allows us to compare performance of different models and helps in model selection.\n",
    "\n",
    "In this task, we will explore following metrics:\n",
    "- confusion matrix\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f1 score\n",
    "\n",
    "#### Dataset\n",
    "The training dataset is available at \"data/ozone_levels_train.csv\" in the respective challenge' repo.<br>\n",
    "The testing dataset is available at \"data/ozone_levels_test.csv\" in the respective challenge' repo.<br>\n",
    "\n",
    "The dataset is __modified version__ of the dataset 'ozone level' on provided by UCI Machine Learning repository.\n",
    "\n",
    "Original dataset: https://archive.ics.uci.edu/ml/datasets/Ozone+Level+Detection\n",
    "\n",
    "#### Objective\n",
    "To learn about classification metrics and compare logistic regression and decision tree on the same dataset\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "#### Further fun\n",
    "\n",
    "#### Helpful links\n",
    "- Classification metrics with google developers: https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Uncomment below 2 lines to ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test data\n",
    "train = \n",
    "test  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_train = \n",
    "X_test  =\n",
    "y_train = \n",
    "y_test  ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "# Classifier 1 - Logistic regression\n",
    "clf1 = \n",
    "# Classifier 2 - Decision tree\n",
    "clf2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing data\n",
    "y_pred_lr = \n",
    "y_pred_dt = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary building blocks of classification metrics\n",
    "\n",
    "A __TRUE POSITIVE (TP)__ is an outcome where the model correctly predicts the positive class.\n",
    "\n",
    "A __TRUE NEGATIVE (TN)__ is an outcome where the model correctly predicts the negative class.\n",
    "\n",
    "A __FALSE POSITIVE (FP)__ is an outcome where the model incorrectly predicts the positive class.\n",
    "\n",
    "a __FALSE NEGATIVE (FN)__ is an outcome where the model incorrectly predicts the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute primary metrics for logisitc regression\n",
    "# True Positive\n",
    "lr_true_positive = \n",
    "# True Negative\n",
    "lr_true_negative = \n",
    "# False Positive\n",
    "lr_false_positive = \n",
    "# False Negative\n",
    "lr_false_negative = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute primary metrics for decision tree\n",
    "# True Positive\n",
    "dt_true_positive = \n",
    "# True Negative\n",
    "dt_true_negative = \n",
    "# False Positive\n",
    "dt_false_positive = \n",
    "# False Negative\n",
    "dt_false_negative = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "A confusion matrix is a technique for summarizing the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Draw confusion matrix here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "Classification accuracy is simply the rate of correct classifications\n",
    "$$Accuracy = \\frac{Number \\, of \\, correct \\, predictions}{Total \\, number \\, of \\, predictions}$$\n",
    "<br>\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy for logistic regression\n",
    "lr_accuracy = \n",
    "# Classification accuracy for decision tree\n",
    "dt_accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classificaton accuracy: LR = \" , lr_accuracy)\n",
    "print(\"Classificaton accuracy: DT = \" , dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "What proportion of positive identifications was actually correct?\n",
    "$$Precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision for logistic regression\n",
    "lr_precision = \n",
    "# Precision for decision tree\n",
    "dt_precision = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: LR = \" , lr_precision)\n",
    "print(\"Precision: DT = \" , dt_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "What proportion of actual positives was identified correctly?\n",
    "$$Recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall for logistic regression\n",
    "lr_recall = \n",
    "# Recall for decision tree\n",
    "dt_recall = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall: LR = \" , lr_recall)\n",
    "print(\"Recall: DT = \" , dt_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "$$ F1 \\, score = \\frac{2* Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score for logistic regression\n",
    "lr_f1_score = \n",
    "# F1 score for decision tree\n",
    "dt_f1_score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score: LR = \" , lr_f1_score)\n",
    "print(\"F1 score: DT = \" , dt_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve\n",
    "A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The area under the curve is then the approximate integral under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rate for logisitic regression\n",
    "lr_false_positive_rate = \n",
    "# True positive rate for logisitic regression\n",
    "lr_true_positive_rate = \n",
    "# False positive rate for decision trees\n",
    "dt_false_positive_rate = \n",
    "# True positive rate for decision trees\n",
    "dt_true_positive_rate = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the ROC curve\n",
    "plt.plot()\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
